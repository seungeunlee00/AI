{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoCn6qvYFqp0fGvWh78eda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seungeunlee00/JUNIA/blob/main/AML/Multi_Layer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Layer Perceptron MLP algorithm (forward backward propagation) - Stochastic Gradient Descent\n",
        "XOR function\n",
        "1. my code\n"
      ],
      "metadata": {
        "id": "x3dmgxEF643y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlJgRgN96zuJ",
        "outputId": "b7fe1b55-c834-49e7-a77c-1e10ad894939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(x0, x1, c) = ( 0 , 0 , 0 )\n",
            "s1= [0.1, 0.5]\n",
            "s2= [0.8999999999999999, 1.01]\n",
            "s3= 1.167\n",
            "loss= 1.167 , g= 1.167\n",
            "g2= [0.46680000000000005, 0.8169]\n",
            "g1= [0.72354, 0.38510999999999995]\n",
            "<update weight>\n",
            "w1= [[0.1, 0.2], [0.3, 0.1]]\n",
            "w2= [[0.490664, 0.583662], [0.25332, 0.25332]]\n",
            "w3= [0.18994000000000003, 0.46426599999999996]\n",
            "<update bias>\n",
            "b1= [0.0855292, 0.461489]\n",
            "b2= [0.615976, 0.6349862000000001]\n",
            "b3= -0.17237780000000005\n",
            "\n",
            "(x0, x1, c) = ( 0 , 1 , 1 )\n",
            "s1= [0.3855292, 0.561489]\n",
            "s2= [0.9473776928688, 1.0022413374104002]\n",
            "s3= 0.4728736957376767\n",
            "loss= 0.5271263042623233 , g= 0.5271263042623233\n",
            "g2= [0.10012237023158571, 0.24472682077465177]\n",
            "g1= [0.19196418833428558, 0.08735719706570008]\n",
            "<update weight>\n",
            "w1= [[0.1, 0.2], [0.26160716233314285, 0.08252856058685999]]\n",
            "w2= [[0.4829439805405026, 0.564792132913641], [0.24207647809220742, 0.24207647809220742]]\n",
            "w3= [0.09006245960350062, 0.35860444556638543]\n",
            "<update bias>\n",
            "b1= [0.07072764000856671, 0.4516789789553554]\n",
            "b2= [0.5970052599770889, 0.5859311327693236]\n",
            "b3= -0.22223063272341362\n",
            "\n",
            "(x0, x1, c) = ( 1 , 0 , 1 )\n",
            "s1= [0.17072764000856672, 0.6516789789553554]\n",
            "s2= [0.8372132981033502, 0.8401129127893125]\n",
            "s3= 0.15443908142050547\n",
            "loss= 0.8455609185794946 , g= 0.8455609185794946\n",
            "g2= [0.0761532960718646, 0.30322190439980323]\n",
            "g1= [0.20803512206832675, 0.09183781240570649]\n",
            "<update weight>\n",
            "w1= [[0.058392975586334654, 0.1816324375188587], [0.26160716233314285, 0.08252856058685999]]\n",
            "w2= [[0.48034368603705796, 0.5544384608862247], [0.2321509776465679, 0.2321509776465679]]\n",
            "w3= [-0.05152050947474679, 0.21653111631666025]\n",
            "<update bias>\n",
            "b1= [0.0636241709226428, 0.43970922459174655]\n",
            "b2= [0.5842539495439355, 0.5349830053039554]\n",
            "b3= -0.24834816303351281\n",
            "\n",
            "(x0, x1, c) = ( 1 , 1 , 0 )\n",
            "s1= [0.3836243088421203, 0.7038702226974652]\n",
            "s2= [0.9319296244421023, 0.9110832369924463]\n",
            "s3= 0\n",
            "loss= 0 , g= 0\n",
            "g2= [-0.0, 0.0]\n",
            "g1= [0.0, 0.0]\n",
            "<update weight>\n",
            "w1= [[0.058392975586334654, 0.1816324375188587], [0.26160716233314285, 0.08252856058685999]]\n",
            "w2= [[0.48034368603705796, 0.5544384608862247], [0.2321509776465679, 0.2321509776465679]]\n",
            "w3= [-0.05152050947474679, 0.21653111631666025]\n",
            "<update bias>\n",
            "b1= [0.0636241709226428, 0.43970922459174655]\n",
            "b2= [0.5842539495439355, 0.5349830053039554]\n",
            "b3= -0.24834816303351281\n"
          ]
        }
      ],
      "source": [
        "# training base S(X,c)\n",
        "x=[[0,0], [0,1], [1,0], [1,1]] # input\n",
        "c=[0,1,1,0] # result of XOR \n",
        "o=[0,0,0,0] # result of training\n",
        "\n",
        "w1=[[0.1, 0.2], [0.3, 0.1]] # weight\n",
        "w2=[[0.5, 0.6], [0.3, 0.3]] \n",
        "w3=[0.4, 0.7] \n",
        "b1=[0.1, 0.5] # bias\n",
        "b2=[0.7, 0.8]\n",
        "b3=0.1\n",
        "n=0.2 # learning rate (클수록 빨리 예측)\n",
        "\n",
        "s1=[0,0] # sigma\n",
        "s2=[0,0]\n",
        "g1=[0,0] # gradient\n",
        "g2=[0,0]\n",
        "\n",
        "# training\n",
        "# repeat \n",
        "\n",
        "# take each example (X,c) from S\n",
        "for i in range(len(x)):\n",
        "  print(\"\\n(x0, x1, c) = (\",x[i][0],\",\",x[i][1],\",\",c[i],\")\")\n",
        "\n",
        "  # calculate s(xi) for each hidden neurone i\n",
        "  s1[0] = max(x[i][0]*w1[0][0] + x[i][1]*w1[1][0] + b1[0], 0) # Relu\n",
        "  s1[1] = max(x[i][0]*w1[0][1] + x[i][1]*w1[1][1] + b1[1], 0) # Relu\n",
        "  print(\"s1=\",s1)\n",
        "  s2[0] = max(s1[0]*w2[0][0] + s1[1]*w2[1][0] + b2[0], 0) # Relu\n",
        "  s2[1] = max(s1[0]*w2[0][1] + s1[1]*w2[1][1] + b2[1], 0) # Relu\n",
        "  print(\"s2=\",s2)\n",
        "  # calculate the output\n",
        "  s3 = max(s2[0]*w3[0] + s2[1]*w3[1] + b3, 0) # Relu\n",
        "  print(\"s3=\",s3)\n",
        "  # calculate the loss and gradient of output\n",
        "  loss=abs(c[i]-s3)\n",
        "  g=1*loss\n",
        "  print(\"loss=\",loss,\", g=\",g)\n",
        "\n",
        "  # backpropagation\n",
        "  g2[0] = 1 * g*w3[0]\n",
        "  g2[1] = 1 * g*w3[1]\n",
        "  print(\"g2=\",g2)\n",
        "  g1[0] = 1 * (g2[0]*w2[0][0] + g2[1]*w2[0][1])\n",
        "  g1[1] = 1 * (g2[0]*w2[1][0] + g2[1]*w2[1][1])\n",
        "  print(\"g1=\",g1)\n",
        "\n",
        "  # update weights based on the gradient\n",
        "  w1[0][0] = w1[0][0] - 0.2*g1[0]*x[i][0]\n",
        "  w1[0][1] = w1[0][1] - 0.2*g1[1]*x[i][0]\n",
        "  w1[1][0] = w1[1][0] - 0.2*g1[0]*x[i][1]\n",
        "  w1[1][1] = w1[1][1] - 0.2*g1[1]*x[i][1]\n",
        "  w2[0][0] = w2[0][0] - 0.2*g2[0]*s1[0]\n",
        "  w2[0][1] = w2[0][1] - 0.2*g2[1]*s1[0]\n",
        "  w2[1][0] = w2[1][0] - 0.2*g2[0]*s1[1]\n",
        "  w2[1][1] = w2[1][1] - 0.2*g2[0]*s1[1]\n",
        "  w3[0] = w3[0] - 0.2*g*s2[0]\n",
        "  w3[1] = w3[1] - 0.2*g*s2[1]\n",
        "  print('<update weight>')\n",
        "  print('w1=',w1)\n",
        "  print('w2=',w2)\n",
        "  print('w3=',w3)\n",
        "\n",
        "  # update bias based on the gradient\n",
        "  b1[0] = b1[0] - 0.2*s1[0]*g1[0]\n",
        "  b1[1] = b1[1] - 0.2*s1[1]*g1[1]\n",
        "  b2[0] = b2[0] - 0.2*s2[0]*g2[0]\n",
        "  b2[1] = b2[1] - 0.2*s2[1]*g2[1]\n",
        "  b3 = b3 - 0.2*s3*g\n",
        "  print('<update bias>')\n",
        "  print('b1=',b1)\n",
        "  print('b2=',b2)\n",
        "  print('b3=',b3)\n",
        "\n",
        "# end repeat when covering all examples from S\n",
        "# calculate the MSE (if MSE =! 0 and total epochs not reached) then go to repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Solution"
      ],
      "metadata": {
        "id": "c4zX7MJ7bPWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Author: H. Benhabiles\n",
        "# Version date: January 2022\n",
        "\"\"\" MLP or fully connected neural network with dynamic Structure developped from scratch\n",
        "This code example is provided in the frame of the Advanced Machine Learning JUNIA-M1 course. \n",
        "It demonstrates how to build an MLP and code it from scratch. The code permits to solve logic functions but \n",
        "can be easily adapted to sovle non linear separated data. It offers the possibility to build different MLP structures by \n",
        "changing dynamically some hyperparameters: the depth of the MLP (number of hidden layers),\n",
        "the size of each layer (number of neurones), activation functions, optimizers and kernel weights initializers.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    if x <0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def tanh(x):\n",
        "    return math.tanh(x)\n",
        "\n",
        "sigmoid_v = np.vectorize(sigmoid)\n",
        "relu_v = np.vectorize(relu)\n",
        "tanh_v = np.vectorize(tanh)\n",
        "sqrt_v = np.vectorize(math.sqrt)\n",
        "log_v = np.vectorize(math.log)\n",
        "\n",
        "def activation_fct (x, name):\n",
        "    if name == 'sigmoid':\n",
        "        return sigmoid_v(x)\n",
        "    elif name == 'relu':\n",
        "        return relu_v(x)\n",
        "    elif name == 'tanh':\n",
        "        return tanh_v(x)\n",
        "\n",
        "def der_activation_fct (x, name):\n",
        "    if name == 'sigmoid':\n",
        "        return x*(1-x)\n",
        "    elif name == 'relu':\n",
        "        return relu_v(x) #np.full(x.shape,1)\n",
        "    elif name == 'tanh':\n",
        "        return 1-(x*x)\n",
        "\n",
        "# loss for classification problem with softmax\n",
        "def entropy_fct(c,o):\n",
        "    loss = 0\n",
        "    for i in range (c.shape[0]):\n",
        "        if c[i] == 0:\n",
        "            loss = loss + abs(math.log((1-o[i])+0.00000001)) # add epsilon to avoid undefined values \n",
        "        else:\n",
        "            loss = loss + abs(math.log(o[i]+0.00000001))\n",
        "    return loss/c.shape[0]\n",
        "\n",
        "# change the order of appearance of the 4 examples\n",
        "def toshuffle(X,C, shuffle):\n",
        "    if shuffle:\n",
        "        # shuffle X and C same way\n",
        "        randomize = np.arange(len(C))\n",
        "        np.random.shuffle(randomize)\n",
        "        X = X[randomize]\n",
        "        C = C[randomize]\n",
        "    return X,C\n",
        "\n",
        "def model_setinput(inputdim, dense, activation, initializer):\n",
        "    model = []\n",
        "    # first hidden layer (weights and baias)\n",
        "    #W = np.random.uniform(-w_range,w_range,[inputdim,dense])\n",
        "    if initializer == 'normal':\n",
        "        W = np.random.normal(0,1,[inputdim,dense])\n",
        "        B = np.zeros([dense,1])\n",
        "    elif initializer == 'constant':\n",
        "        W = np.full((inputdim,dense), 0.05)\n",
        "        B = np.full((dense,1),0.05)\n",
        "    elif initializer == 'uniform': \n",
        "        W = np.random.uniform(-0.05,0.05,[inputdim,dense])\n",
        "        B = np.random.uniform(-0.05,0.05,[dense,1])\n",
        "    #elif initializer == 'glorot': #### to do \n",
        "    # associate weights to the dense layer\n",
        "    dense_weight =[]\n",
        "    dense_weight.append(W)\n",
        "    dense_weight.append(B)\n",
        "    dense_weight.append(activation)\n",
        "    model.append(dense_weight)\n",
        "    return model\n",
        "\n",
        "def model_addlayer(model, dense, activation, initializer):\n",
        "    dense_previous = model[-1][1].shape[0] # size of length of last hidden layer\n",
        "    #hidden layer (weights and baias)\n",
        "    #W = np.random.uniform(-w_range,w_range,[dense_previous,dense])\n",
        "    if initializer == 'normal':\n",
        "        W = np.random.normal(0,1,[dense_previous,dense])\n",
        "        B = np.zeros([dense,1])\n",
        "    elif initializer == 'constant':\n",
        "        W = np.full((dense_previous,dense),0.5)\n",
        "        B = np.full((dense,1),0.5)\n",
        "    elif initializer == 'uniform': \n",
        "        W = np.random.uniform(-0.5,0.5,[dense_previous,dense])\n",
        "        B = np.random.uniform(-0.5,0.5,[dense,1])\n",
        "    # associate weights to the dense layer\n",
        "    dense_weight =[]\n",
        "    dense_weight.append(W)\n",
        "    dense_weight.append(B)\n",
        "    dense_weight.append(activation)\n",
        "    model.append(dense_weight)\n",
        "    return model\n",
        "\n",
        "def model_sgd(model, gradient_list, activation_list, lr, x):\n",
        "    # need to update first layer of weights alone since linked to input data.\n",
        "    gradient     = gradient_list[0].T*x.T\n",
        "    model[0][0]  = model[0][0]  - lr*gradient  # weights \n",
        "    model[0][1] = model[0][1] - lr*gradient_list[0] # baias\n",
        "    for layer_index in range (1,len(model)):\n",
        "        gradient     = gradient_list[layer_index].T*activation_list[layer_index-1]\n",
        "        model[layer_index][0]  = model[layer_index][0]  - lr*gradient # weights \n",
        "        model[layer_index][1] = model[layer_index][1] - lr*gradient_list[layer_index] # baias\n",
        "    return model\n",
        "\n",
        "def model_adam(model, gradient_list, m, v, lr):\n",
        "    beta_1 = 0.9\n",
        "    beta_2 = 0.999\n",
        "    epsilon = 1/pow(10,8)\n",
        "    if not m and not v:\n",
        "        # define m and v with same shape as gradient_list and set their units to 0\n",
        "        for i in range (len(gradient_list)):\n",
        "            m.append(np.zeros(gradient_list[i].shape))\n",
        "            v.append(np.zeros(gradient_list[i].shape))\n",
        "    # calculate moments and update weights\n",
        "    for layer_index in range (len(gradient_list)):\n",
        "        m[layer_index] = beta_1*m[layer_index] + (1-beta_1)*gradient_list[layer_index]\n",
        "        v[layer_index] = beta_2*v[layer_index] + ((1-beta_2)*gradient_list[layer_index]*gradient_list[layer_index])\n",
        "        adam_gradient = (lr*m[layer_index])/(sqrt_v(v[layer_index])+epsilon)\n",
        "        model[layer_index][0] = model[layer_index][0] - adam_gradient.T # weights\n",
        "        model[layer_index][1] = model[layer_index][1] - adam_gradient # baias \n",
        "    return model, m, v\n",
        "\n",
        "def model_train(name, model, X,C, optimizer, lr, loss_fct, num_epochs, shuffle):\n",
        "    solved = False\n",
        "    # set initially to positive infinity, we suppose that the best error at this stage is too high\n",
        "    best_error = float('inf')\n",
        "    if optimizer == 'adam':\n",
        "        m = []\n",
        "        v = []\n",
        "    for epochs in range(num_epochs):\n",
        "        # We may shuffle the data if required\n",
        "        X,C = toshuffle(X,C,shuffle)\n",
        "        for example_index in range (len(C)): \n",
        "            x = X[example_index].reshape(1,2)\n",
        "            # setup activation and gradient lists.\n",
        "            activation_list = []\n",
        "            gradient_list = []\n",
        "            ######## feed pass or propagation #############\n",
        "            W = model[0][0]\n",
        "            B = model[0][1]\n",
        "            transfer = x.dot(W).T+B # first hidden layer\n",
        "            activation =  activation_fct(transfer, model[0][2]) \n",
        "            activation_list.append(activation)\n",
        "            for layer_index in range (1,len(model)): # next layers\n",
        "                # calculate activation function\n",
        "                W = model[layer_index][0]\n",
        "                B = model[layer_index][1]\n",
        "                transfer = activation_list[-1].T.dot(W).T+B # next hidden layer\n",
        "                activation = activation_fct(transfer, model[layer_index][2])\n",
        "                activation_list.append(activation)\n",
        "            output = activation_list[-1]\n",
        "            if loss_fct == 'mse':\n",
        "                loss  = abs(C[example_index].reshape(1,1) - output)\n",
        "            elif loss_fct == 'entropy':\n",
        "                if C[example_index] == 0:\n",
        "                    loss = abs(math.log(1-output+0.00000001))\n",
        "                else:\n",
        "                    loss = abs(math.log(output+0.00000001))\n",
        "            gradient_output  = der_activation_fct(output, model[-1][2]).dot(loss)\n",
        "            gradient_list.append(gradient_output)\n",
        "            ######## backpropagation #############\n",
        "            for layer_index in range (len(model)-1, 0, -1):\n",
        "                W = model[layer_index][0]\n",
        "                gradient = der_activation_fct(activation_list[layer_index-1], model[layer_index-1][2])*(W.dot(gradient_list[-1]))\n",
        "                gradient_list.append(gradient)\n",
        "            ######## update weights #############\n",
        "            # gradient_list.reverse() to align with layers orders\n",
        "            gradient_list.reverse()\n",
        "            ######## optimizer call to update weights #############\n",
        "            if optimizer == 'sgd': \n",
        "                model = model_sgd(model, gradient_list, activation_list, lr, x)\n",
        "            elif optimizer == 'adam':\n",
        "                model, m, v = model_adam(model, gradient_list, m, v, lr)\n",
        "        # prepare saving the best model\n",
        "        O = model_predict(model, X, False)\n",
        "        if loss_fct == 'mse':\n",
        "            # calculate MSE the Mean Squared Error\n",
        "            error = np.average(pow((C-O),2))\n",
        "        elif loss_fct == 'entropy':\n",
        "            error = entropy_fct(C,O)\n",
        "        if error < best_error:\n",
        "            best_error = error\n",
        "            # save model (weights and baias)\n",
        "            best_model = model\n",
        "        if error > 1: # if big error stop iterating over epochs, it will not converge\n",
        "            break\n",
        "        print(\"epoch, \", loss_fct, \": \", epochs, error)\n",
        "        if np.average(pow((C-np.round(O)),2)) == 0:\n",
        "            print(name, \"solved!\")\n",
        "            print(epochs+1, \" epoch(s)\")\n",
        "            print (\"C,O \", C, O)\n",
        "            solved = True\n",
        "            break\n",
        "    return solved, best_model\n",
        "    \n",
        "def model_predict(model, X, toround):\n",
        "    O = np.empty(X.shape[0])\n",
        "    for example_index in range (len(X)): \n",
        "        x = X[example_index].reshape(1,2)\n",
        "        # setup activation\n",
        "        activation_list = []\n",
        "        ######## propagation #############\n",
        "        W = model[0][0]\n",
        "        B = model[0][1]\n",
        "        transfer = x.dot(W).T+B # first hidden layer\n",
        "        activation =  activation_fct(transfer, model[0][2])\n",
        "        activation_list.append(activation)\n",
        "        for layer_index in range (1,len(model)): # hidden layers\n",
        "            # calculate activation function\n",
        "            W = model[layer_index][0]\n",
        "            B = model[layer_index][1]\n",
        "            transfer = activation_list[-1].T.dot(W).T+B # next hidden layer\n",
        "            activation = activation_fct(transfer, model[layer_index][2])\n",
        "            activation_list.append(activation)\n",
        "        O[example_index] = activation_list[-1] \n",
        "    if (toround):\n",
        "        return np.round(O)\n",
        "    else:\n",
        "        return O"
      ],
      "metadata": {
        "id": "W8CxmDgYbO6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# call MLP train function #################  \n",
        "    \n",
        "run = 0\n",
        "X = np.array(([0,0],[0,1],[1,0],[1,1]), dtype=int)#inputs\n",
        "C = np.array((0,1,1,0),dtype=int)#resultat d'un XOR\n",
        "lr = 0.001\n",
        "num_epochs = 50\n",
        "max_run = 0 \n",
        "\n",
        "solved = False\n",
        "while (True): # max_run < 25\n",
        "    \n",
        "    max_run = max_run+1\n",
        "    print (\"====Run {}====\".format(max_run))\n",
        "    # Build a new model\n",
        "    model = model_setinput(2, 8, 'relu', initializer='normal')\n",
        "    model = model_addlayer(model, 4,'relu', initializer='normal')\n",
        "    model = model_addlayer(model, 2,'relu', initializer='normal')\n",
        "    model = model_addlayer(model, 1,'relu', initializer='normal')\n",
        "    solved, best_model = model_train(\"XOR\", model, X,C,'sgd',lr, 'mse', num_epochs, shuffle=False)\n",
        "    if solved:\n",
        "        print (\"solved :) ---- Congratulation\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3lffdGTbsex",
        "outputId": "613e8360-a5a9-4421-be4a-da8b936d0faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====Run 1====\n",
            "epoch,  mse :  0 0.5\n",
            "epoch,  mse :  1 0.5\n",
            "epoch,  mse :  2 0.5\n",
            "epoch,  mse :  3 0.5\n",
            "epoch,  mse :  4 0.5\n",
            "epoch,  mse :  5 0.5\n",
            "epoch,  mse :  6 0.5\n",
            "epoch,  mse :  7 0.5\n",
            "epoch,  mse :  8 0.5\n",
            "epoch,  mse :  9 0.5\n",
            "epoch,  mse :  10 0.5\n",
            "epoch,  mse :  11 0.5\n",
            "epoch,  mse :  12 0.5\n",
            "epoch,  mse :  13 0.5\n",
            "epoch,  mse :  14 0.5\n",
            "epoch,  mse :  15 0.5\n",
            "epoch,  mse :  16 0.5\n",
            "epoch,  mse :  17 0.5\n",
            "epoch,  mse :  18 0.5\n",
            "epoch,  mse :  19 0.5\n",
            "epoch,  mse :  20 0.5\n",
            "epoch,  mse :  21 0.5\n",
            "epoch,  mse :  22 0.5\n",
            "epoch,  mse :  23 0.5\n",
            "epoch,  mse :  24 0.5\n",
            "epoch,  mse :  25 0.5\n",
            "epoch,  mse :  26 0.5\n",
            "epoch,  mse :  27 0.5\n",
            "epoch,  mse :  28 0.5\n",
            "epoch,  mse :  29 0.5\n",
            "epoch,  mse :  30 0.5\n",
            "epoch,  mse :  31 0.5\n",
            "epoch,  mse :  32 0.5\n",
            "epoch,  mse :  33 0.5\n",
            "epoch,  mse :  34 0.5\n",
            "epoch,  mse :  35 0.5\n",
            "epoch,  mse :  36 0.5\n",
            "epoch,  mse :  37 0.5\n",
            "epoch,  mse :  38 0.5\n",
            "epoch,  mse :  39 0.5\n",
            "epoch,  mse :  40 0.5\n",
            "epoch,  mse :  41 0.5\n",
            "epoch,  mse :  42 0.5\n",
            "epoch,  mse :  43 0.5\n",
            "epoch,  mse :  44 0.5\n",
            "epoch,  mse :  45 0.5\n",
            "epoch,  mse :  46 0.5\n",
            "epoch,  mse :  47 0.5\n",
            "epoch,  mse :  48 0.5\n",
            "epoch,  mse :  49 0.5\n",
            "====Run 2====\n",
            "epoch,  mse :  0 0.75\n",
            "epoch,  mse :  1 0.75\n",
            "epoch,  mse :  2 0.75\n",
            "epoch,  mse :  3 0.75\n",
            "epoch,  mse :  4 0.75\n",
            "epoch,  mse :  5 0.75\n",
            "epoch,  mse :  6 0.75\n",
            "epoch,  mse :  7 0.75\n",
            "epoch,  mse :  8 0.75\n",
            "epoch,  mse :  9 0.75\n",
            "epoch,  mse :  10 0.75\n",
            "epoch,  mse :  11 0.75\n",
            "epoch,  mse :  12 0.75\n",
            "epoch,  mse :  13 0.75\n",
            "epoch,  mse :  14 0.75\n",
            "epoch,  mse :  15 0.75\n",
            "epoch,  mse :  16 0.75\n",
            "epoch,  mse :  17 0.75\n",
            "epoch,  mse :  18 0.75\n",
            "epoch,  mse :  19 0.75\n",
            "epoch,  mse :  20 0.75\n",
            "epoch,  mse :  21 0.75\n",
            "epoch,  mse :  22 0.75\n",
            "epoch,  mse :  23 0.75\n",
            "epoch,  mse :  24 0.75\n",
            "epoch,  mse :  25 0.75\n",
            "epoch,  mse :  26 0.75\n",
            "epoch,  mse :  27 0.75\n",
            "epoch,  mse :  28 0.75\n",
            "epoch,  mse :  29 0.75\n",
            "epoch,  mse :  30 0.75\n",
            "epoch,  mse :  31 0.75\n",
            "epoch,  mse :  32 0.75\n",
            "epoch,  mse :  33 0.75\n",
            "epoch,  mse :  34 0.75\n",
            "epoch,  mse :  35 0.75\n",
            "epoch,  mse :  36 0.75\n",
            "epoch,  mse :  37 0.75\n",
            "epoch,  mse :  38 0.75\n",
            "epoch,  mse :  39 0.75\n",
            "epoch,  mse :  40 0.75\n",
            "epoch,  mse :  41 0.75\n",
            "epoch,  mse :  42 0.75\n",
            "epoch,  mse :  43 0.75\n",
            "epoch,  mse :  44 0.75\n",
            "epoch,  mse :  45 0.75\n",
            "epoch,  mse :  46 0.75\n",
            "epoch,  mse :  47 0.75\n",
            "epoch,  mse :  48 0.75\n",
            "epoch,  mse :  49 0.75\n",
            "====Run 3====\n",
            "epoch,  mse :  0 0.5\n",
            "epoch,  mse :  1 0.5\n",
            "epoch,  mse :  2 0.5\n",
            "epoch,  mse :  3 0.5\n",
            "epoch,  mse :  4 0.5\n",
            "epoch,  mse :  5 0.5\n",
            "epoch,  mse :  6 0.5\n",
            "epoch,  mse :  7 0.5\n",
            "epoch,  mse :  8 0.5\n",
            "epoch,  mse :  9 0.5\n",
            "epoch,  mse :  10 0.5\n",
            "epoch,  mse :  11 0.5\n",
            "epoch,  mse :  12 0.5\n",
            "epoch,  mse :  13 0.5\n",
            "epoch,  mse :  14 0.5\n",
            "epoch,  mse :  15 0.5\n",
            "epoch,  mse :  16 0.5\n",
            "epoch,  mse :  17 0.5\n",
            "epoch,  mse :  18 0.5\n",
            "epoch,  mse :  19 0.5\n",
            "epoch,  mse :  20 0.5\n",
            "epoch,  mse :  21 0.5\n",
            "epoch,  mse :  22 0.5\n",
            "epoch,  mse :  23 0.5\n",
            "epoch,  mse :  24 0.5\n",
            "epoch,  mse :  25 0.5\n",
            "epoch,  mse :  26 0.5\n",
            "epoch,  mse :  27 0.5\n",
            "epoch,  mse :  28 0.5\n",
            "epoch,  mse :  29 0.5\n",
            "epoch,  mse :  30 0.5\n",
            "epoch,  mse :  31 0.5\n",
            "epoch,  mse :  32 0.5\n",
            "epoch,  mse :  33 0.5\n",
            "epoch,  mse :  34 0.5\n",
            "epoch,  mse :  35 0.5\n",
            "epoch,  mse :  36 0.5\n",
            "epoch,  mse :  37 0.5\n",
            "epoch,  mse :  38 0.5\n",
            "epoch,  mse :  39 0.5\n",
            "epoch,  mse :  40 0.5\n",
            "epoch,  mse :  41 0.5\n",
            "epoch,  mse :  42 0.5\n",
            "epoch,  mse :  43 0.5\n",
            "epoch,  mse :  44 0.5\n",
            "epoch,  mse :  45 0.5\n",
            "epoch,  mse :  46 0.5\n",
            "epoch,  mse :  47 0.5\n",
            "epoch,  mse :  48 0.5\n",
            "epoch,  mse :  49 0.5\n",
            "====Run 4====\n",
            "epoch,  mse :  0 0.5\n",
            "epoch,  mse :  1 0.5\n",
            "epoch,  mse :  2 0.5\n",
            "epoch,  mse :  3 0.5\n",
            "epoch,  mse :  4 0.5\n",
            "epoch,  mse :  5 0.5\n",
            "epoch,  mse :  6 0.5\n",
            "epoch,  mse :  7 0.5\n",
            "epoch,  mse :  8 0.5\n",
            "epoch,  mse :  9 0.5\n",
            "epoch,  mse :  10 0.5\n",
            "epoch,  mse :  11 0.5\n",
            "epoch,  mse :  12 0.5\n",
            "epoch,  mse :  13 0.5\n",
            "epoch,  mse :  14 0.5\n",
            "epoch,  mse :  15 0.5\n",
            "epoch,  mse :  16 0.5\n",
            "epoch,  mse :  17 0.5\n",
            "epoch,  mse :  18 0.5\n",
            "epoch,  mse :  19 0.5\n",
            "epoch,  mse :  20 0.5\n",
            "epoch,  mse :  21 0.5\n",
            "epoch,  mse :  22 0.5\n",
            "epoch,  mse :  23 0.5\n",
            "epoch,  mse :  24 0.5\n",
            "epoch,  mse :  25 0.5\n",
            "epoch,  mse :  26 0.5\n",
            "epoch,  mse :  27 0.5\n",
            "epoch,  mse :  28 0.5\n",
            "epoch,  mse :  29 0.5\n",
            "epoch,  mse :  30 0.5\n",
            "epoch,  mse :  31 0.5\n",
            "epoch,  mse :  32 0.5\n",
            "epoch,  mse :  33 0.5\n",
            "epoch,  mse :  34 0.5\n",
            "epoch,  mse :  35 0.5\n",
            "epoch,  mse :  36 0.5\n",
            "epoch,  mse :  37 0.5\n",
            "epoch,  mse :  38 0.5\n",
            "epoch,  mse :  39 0.5\n",
            "epoch,  mse :  40 0.5\n",
            "epoch,  mse :  41 0.5\n",
            "epoch,  mse :  42 0.5\n",
            "epoch,  mse :  43 0.5\n",
            "epoch,  mse :  44 0.5\n",
            "epoch,  mse :  45 0.5\n",
            "epoch,  mse :  46 0.5\n",
            "epoch,  mse :  47 0.5\n",
            "epoch,  mse :  48 0.5\n",
            "epoch,  mse :  49 0.5\n",
            "====Run 5====\n",
            "epoch,  mse :  0 0.5\n",
            "epoch,  mse :  1 0.5\n",
            "epoch,  mse :  2 0.5\n",
            "epoch,  mse :  3 0.5\n",
            "epoch,  mse :  4 0.5\n",
            "epoch,  mse :  5 0.5\n",
            "epoch,  mse :  6 0.5\n",
            "epoch,  mse :  7 0.5\n",
            "epoch,  mse :  8 0.5\n",
            "epoch,  mse :  9 0.5\n",
            "epoch,  mse :  10 0.5\n",
            "epoch,  mse :  11 0.5\n",
            "epoch,  mse :  12 0.5\n",
            "epoch,  mse :  13 0.5\n",
            "epoch,  mse :  14 0.5\n",
            "epoch,  mse :  15 0.5\n",
            "epoch,  mse :  16 0.5\n",
            "epoch,  mse :  17 0.5\n",
            "epoch,  mse :  18 0.5\n",
            "epoch,  mse :  19 0.5\n",
            "epoch,  mse :  20 0.5\n",
            "epoch,  mse :  21 0.5\n",
            "epoch,  mse :  22 0.5\n",
            "epoch,  mse :  23 0.5\n",
            "epoch,  mse :  24 0.5\n",
            "epoch,  mse :  25 0.5\n",
            "epoch,  mse :  26 0.5\n",
            "epoch,  mse :  27 0.5\n",
            "epoch,  mse :  28 0.5\n",
            "epoch,  mse :  29 0.5\n",
            "epoch,  mse :  30 0.5\n",
            "epoch,  mse :  31 0.5\n",
            "epoch,  mse :  32 0.5\n",
            "epoch,  mse :  33 0.5\n",
            "epoch,  mse :  34 0.5\n",
            "epoch,  mse :  35 0.5\n",
            "epoch,  mse :  36 0.5\n",
            "epoch,  mse :  37 0.5\n",
            "epoch,  mse :  38 0.5\n",
            "epoch,  mse :  39 0.5\n",
            "epoch,  mse :  40 0.5\n",
            "epoch,  mse :  41 0.5\n",
            "epoch,  mse :  42 0.5\n",
            "epoch,  mse :  43 0.5\n",
            "epoch,  mse :  44 0.5\n",
            "epoch,  mse :  45 0.5\n",
            "epoch,  mse :  46 0.5\n",
            "epoch,  mse :  47 0.5\n",
            "epoch,  mse :  48 0.5\n",
            "epoch,  mse :  49 0.5\n",
            "====Run 6====\n",
            "epoch,  mse :  0 0.5\n",
            "epoch,  mse :  1 0.5\n",
            "epoch,  mse :  2 0.5\n",
            "epoch,  mse :  3 0.5\n",
            "epoch,  mse :  4 0.5\n",
            "epoch,  mse :  5 0.5\n",
            "epoch,  mse :  6 0.5\n",
            "epoch,  mse :  7 0.5\n",
            "epoch,  mse :  8 0.5\n",
            "epoch,  mse :  9 0.5\n",
            "epoch,  mse :  10 0.5\n",
            "epoch,  mse :  11 0.5\n",
            "epoch,  mse :  12 0.5\n",
            "epoch,  mse :  13 0.5\n",
            "epoch,  mse :  14 0.5\n",
            "epoch,  mse :  15 0.5\n",
            "epoch,  mse :  16 0.5\n",
            "epoch,  mse :  17 0.5\n",
            "epoch,  mse :  18 0.5\n",
            "epoch,  mse :  19 0.5\n",
            "epoch,  mse :  20 0.5\n",
            "epoch,  mse :  21 0.5\n",
            "epoch,  mse :  22 0.5\n",
            "epoch,  mse :  23 0.5\n",
            "epoch,  mse :  24 0.5\n",
            "epoch,  mse :  25 0.5\n",
            "epoch,  mse :  26 0.5\n",
            "epoch,  mse :  27 0.5\n",
            "epoch,  mse :  28 0.5\n",
            "epoch,  mse :  29 0.5\n",
            "epoch,  mse :  30 0.5\n",
            "epoch,  mse :  31 0.5\n",
            "epoch,  mse :  32 0.5\n",
            "epoch,  mse :  33 0.5\n",
            "epoch,  mse :  34 0.5\n",
            "epoch,  mse :  35 0.5\n",
            "epoch,  mse :  36 0.5\n",
            "epoch,  mse :  37 0.5\n",
            "epoch,  mse :  38 0.5\n",
            "epoch,  mse :  39 0.5\n",
            "epoch,  mse :  40 0.5\n",
            "epoch,  mse :  41 0.5\n",
            "epoch,  mse :  42 0.5\n",
            "epoch,  mse :  43 0.5\n",
            "epoch,  mse :  44 0.5\n",
            "epoch,  mse :  45 0.5\n",
            "epoch,  mse :  46 0.5\n",
            "epoch,  mse :  47 0.5\n",
            "epoch,  mse :  48 0.5\n",
            "epoch,  mse :  49 0.5\n",
            "====Run 7====\n",
            "epoch,  mse :  0 0.25\n",
            "epoch,  mse :  1 0.25\n",
            "epoch,  mse :  2 0.25\n",
            "epoch,  mse :  3 0.25\n",
            "epoch,  mse :  4 0.25\n",
            "epoch,  mse :  5 0.25\n",
            "epoch,  mse :  6 0.25\n",
            "epoch,  mse :  7 0.25\n",
            "epoch,  mse :  8 0.25\n",
            "epoch,  mse :  9 0.25\n",
            "epoch,  mse :  10 0.25\n",
            "epoch,  mse :  11 0.25\n",
            "epoch,  mse :  12 0.25\n",
            "epoch,  mse :  13 0.25\n",
            "epoch,  mse :  14 0.25\n",
            "epoch,  mse :  15 0.25\n",
            "epoch,  mse :  16 0.25\n",
            "epoch,  mse :  17 0.25\n",
            "epoch,  mse :  18 0.25\n",
            "epoch,  mse :  19 0.25\n",
            "epoch,  mse :  20 0.25\n",
            "epoch,  mse :  21 0.25\n",
            "epoch,  mse :  22 0.25\n",
            "epoch,  mse :  23 0.25\n",
            "epoch,  mse :  24 0.25\n",
            "epoch,  mse :  25 0.25\n",
            "epoch,  mse :  26 0.25\n",
            "epoch,  mse :  27 0.25\n",
            "epoch,  mse :  28 0.25\n",
            "epoch,  mse :  29 0.25\n",
            "epoch,  mse :  30 0.25\n",
            "epoch,  mse :  31 0.25\n",
            "epoch,  mse :  32 0.25\n",
            "epoch,  mse :  33 0.25\n",
            "epoch,  mse :  34 0.25\n",
            "epoch,  mse :  35 0.25\n",
            "epoch,  mse :  36 0.25\n",
            "epoch,  mse :  37 0.25\n",
            "epoch,  mse :  38 0.25\n",
            "epoch,  mse :  39 0.25\n",
            "epoch,  mse :  40 0.25\n",
            "epoch,  mse :  41 0.25\n",
            "epoch,  mse :  42 0.25\n",
            "epoch,  mse :  43 0.25\n",
            "epoch,  mse :  44 0.25\n",
            "epoch,  mse :  45 0.25\n",
            "epoch,  mse :  46 0.25\n",
            "epoch,  mse :  47 0.25\n",
            "epoch,  mse :  48 0.25\n",
            "epoch,  mse :  49 0.25\n",
            "====Run 8====\n",
            "epoch,  mse :  0 0.25\n",
            "epoch,  mse :  1 0.25\n",
            "epoch,  mse :  2 0.25\n",
            "epoch,  mse :  3 0.25\n",
            "epoch,  mse :  4 0.25\n",
            "epoch,  mse :  5 0.25\n",
            "epoch,  mse :  6 0.25\n",
            "epoch,  mse :  7 0.25\n",
            "epoch,  mse :  8 0.25\n",
            "epoch,  mse :  9 0.25\n",
            "epoch,  mse :  10 0.25\n",
            "epoch,  mse :  11 0.25\n",
            "epoch,  mse :  12 0.25\n",
            "epoch,  mse :  13 0.25\n",
            "epoch,  mse :  14 0.25\n",
            "epoch,  mse :  15 0.25\n",
            "epoch,  mse :  16 0.25\n",
            "epoch,  mse :  17 0.25\n",
            "epoch,  mse :  18 0.5\n",
            "epoch,  mse :  19 0.5\n",
            "epoch,  mse :  20 0.5\n",
            "epoch,  mse :  21 0.5\n",
            "epoch,  mse :  22 0.5\n",
            "epoch,  mse :  23 0.5\n",
            "epoch,  mse :  24 0.5\n",
            "epoch,  mse :  25 0.5\n",
            "epoch,  mse :  26 0.5\n",
            "epoch,  mse :  27 0.5\n",
            "epoch,  mse :  28 0.5\n",
            "epoch,  mse :  29 0.5\n",
            "epoch,  mse :  30 0.5\n",
            "epoch,  mse :  31 0.5\n",
            "epoch,  mse :  32 0.5\n",
            "epoch,  mse :  33 0.5\n",
            "epoch,  mse :  34 0.5\n",
            "epoch,  mse :  35 0.5\n",
            "epoch,  mse :  36 0.5\n",
            "epoch,  mse :  37 0.5\n",
            "epoch,  mse :  38 0.5\n",
            "epoch,  mse :  39 0.5\n",
            "epoch,  mse :  40 0.5\n",
            "epoch,  mse :  41 0.5\n",
            "epoch,  mse :  42 0.5\n",
            "epoch,  mse :  43 0.5\n",
            "epoch,  mse :  44 0.5\n",
            "epoch,  mse :  45 0.5\n",
            "epoch,  mse :  46 0.5\n",
            "epoch,  mse :  47 0.5\n",
            "epoch,  mse :  48 0.5\n",
            "epoch,  mse :  49 0.5\n",
            "====Run 9====\n",
            "epoch,  mse :  0 0.5\n",
            "epoch,  mse :  1 0.5\n",
            "epoch,  mse :  2 0.5\n",
            "epoch,  mse :  3 0.5\n",
            "epoch,  mse :  4 0.5\n",
            "epoch,  mse :  5 0.5\n",
            "epoch,  mse :  6 0.5\n",
            "epoch,  mse :  7 0.5\n",
            "epoch,  mse :  8 0.5\n",
            "epoch,  mse :  9 0.75\n",
            "epoch,  mse :  10 0.5\n",
            "epoch,  mse :  11 0.5\n",
            "epoch,  mse :  12 0.5\n",
            "epoch,  mse :  13 0.5\n",
            "epoch,  mse :  14 0.5\n",
            "epoch,  mse :  15 0.5\n",
            "epoch,  mse :  16 0.5\n",
            "epoch,  mse :  17 0.5\n",
            "epoch,  mse :  18 0.5\n",
            "epoch,  mse :  19 0.5\n",
            "epoch,  mse :  20 0.5\n",
            "epoch,  mse :  21 0.5\n",
            "epoch,  mse :  22 0.5\n",
            "epoch,  mse :  23 0.5\n",
            "epoch,  mse :  24 0.5\n",
            "epoch,  mse :  25 0.5\n",
            "epoch,  mse :  26 0.5\n",
            "epoch,  mse :  27 0.5\n",
            "epoch,  mse :  28 0.5\n",
            "epoch,  mse :  29 0.5\n",
            "epoch,  mse :  30 0.5\n",
            "epoch,  mse :  31 0.5\n",
            "epoch,  mse :  32 0.5\n",
            "epoch,  mse :  33 0.5\n",
            "epoch,  mse :  34 0.5\n",
            "epoch,  mse :  35 0.5\n",
            "epoch,  mse :  36 0.5\n",
            "epoch,  mse :  37 0.5\n",
            "epoch,  mse :  38 0.5\n",
            "epoch,  mse :  39 0.5\n",
            "epoch,  mse :  40 0.5\n",
            "epoch,  mse :  41 0.5\n",
            "epoch,  mse :  42 0.5\n",
            "epoch,  mse :  43 0.5\n",
            "epoch,  mse :  44 0.5\n",
            "epoch,  mse :  45 0.5\n",
            "epoch,  mse :  46 0.5\n",
            "epoch,  mse :  47 0.5\n",
            "epoch,  mse :  48 0.5\n",
            "epoch,  mse :  49 0.5\n",
            "====Run 10====\n",
            "epoch,  mse :  0 0.25\n",
            "epoch,  mse :  1 0.25\n",
            "epoch,  mse :  2 0.25\n",
            "epoch,  mse :  3 0.25\n",
            "epoch,  mse :  4 0.25\n",
            "epoch,  mse :  5 0.25\n",
            "epoch,  mse :  6 0.25\n",
            "epoch,  mse :  7 0.25\n",
            "epoch,  mse :  8 0.25\n",
            "epoch,  mse :  9 0.25\n",
            "epoch,  mse :  10 0.25\n",
            "epoch,  mse :  11 0.25\n",
            "epoch,  mse :  12 0.25\n",
            "epoch,  mse :  13 0.25\n",
            "epoch,  mse :  14 0.25\n",
            "epoch,  mse :  15 0.25\n",
            "epoch,  mse :  16 0.25\n",
            "epoch,  mse :  17 0.25\n",
            "epoch,  mse :  18 0.25\n",
            "epoch,  mse :  19 0.25\n",
            "epoch,  mse :  20 0.25\n",
            "epoch,  mse :  21 0.25\n",
            "epoch,  mse :  22 0.25\n",
            "epoch,  mse :  23 0.25\n",
            "epoch,  mse :  24 0.25\n",
            "epoch,  mse :  25 0.25\n",
            "epoch,  mse :  26 0.25\n",
            "epoch,  mse :  27 0.25\n",
            "epoch,  mse :  28 0.25\n",
            "epoch,  mse :  29 0.25\n",
            "epoch,  mse :  30 0.25\n",
            "epoch,  mse :  31 0.25\n",
            "epoch,  mse :  32 0.25\n",
            "epoch,  mse :  33 0.25\n",
            "epoch,  mse :  34 0.25\n",
            "epoch,  mse :  35 0.25\n",
            "epoch,  mse :  36 0.25\n",
            "epoch,  mse :  37 0.25\n",
            "epoch,  mse :  38 0.25\n",
            "epoch,  mse :  39 0.25\n",
            "epoch,  mse :  40 0.25\n",
            "epoch,  mse :  41 0.25\n",
            "epoch,  mse :  42 0.25\n",
            "epoch,  mse :  43 0.25\n",
            "epoch,  mse :  44 0.25\n",
            "epoch,  mse :  45 0.25\n",
            "epoch,  mse :  46 0.25\n",
            "epoch,  mse :  47 0.25\n",
            "epoch,  mse :  48 0.25\n",
            "epoch,  mse :  49 0.25\n",
            "====Run 11====\n",
            "epoch,  mse :  0 0.5\n",
            "epoch,  mse :  1 0.5\n",
            "epoch,  mse :  2 0.5\n",
            "epoch,  mse :  3 0.5\n",
            "epoch,  mse :  4 0.5\n",
            "epoch,  mse :  5 0.5\n",
            "epoch,  mse :  6 0.5\n",
            "epoch,  mse :  7 0.5\n",
            "epoch,  mse :  8 0.5\n",
            "epoch,  mse :  9 0.5\n",
            "epoch,  mse :  10 0.5\n",
            "epoch,  mse :  11 0.5\n",
            "epoch,  mse :  12 0.5\n",
            "epoch,  mse :  13 0.5\n",
            "epoch,  mse :  14 0.5\n",
            "epoch,  mse :  15 0.5\n",
            "epoch,  mse :  16 0.5\n",
            "epoch,  mse :  17 0.5\n",
            "epoch,  mse :  18 0.5\n",
            "epoch,  mse :  19 0.5\n",
            "epoch,  mse :  20 0.5\n",
            "epoch,  mse :  21 0.5\n",
            "epoch,  mse :  22 0.5\n",
            "epoch,  mse :  23 0.5\n",
            "epoch,  mse :  24 0.5\n",
            "epoch,  mse :  25 0.5\n",
            "epoch,  mse :  26 0.5\n",
            "epoch,  mse :  27 0.5\n",
            "epoch,  mse :  28 0.5\n",
            "epoch,  mse :  29 0.5\n",
            "epoch,  mse :  30 0.5\n",
            "epoch,  mse :  31 0.5\n",
            "epoch,  mse :  32 0.5\n",
            "epoch,  mse :  33 0.5\n",
            "epoch,  mse :  34 0.5\n",
            "epoch,  mse :  35 0.5\n",
            "epoch,  mse :  36 0.5\n",
            "epoch,  mse :  37 0.5\n",
            "epoch,  mse :  38 0.5\n",
            "epoch,  mse :  39 0.5\n",
            "epoch,  mse :  40 0.5\n",
            "epoch,  mse :  41 0.5\n",
            "epoch,  mse :  42 0.5\n",
            "epoch,  mse :  43 0.5\n",
            "epoch,  mse :  44 0.5\n",
            "epoch,  mse :  45 0.5\n",
            "epoch,  mse :  46 0.5\n",
            "epoch,  mse :  47 0.5\n",
            "epoch,  mse :  48 0.5\n",
            "epoch,  mse :  49 0.5\n",
            "====Run 12====\n",
            "epoch,  mse :  0 0.5\n",
            "epoch,  mse :  1 0.5\n",
            "epoch,  mse :  2 0.5\n",
            "epoch,  mse :  3 0.5\n",
            "epoch,  mse :  4 0.5\n",
            "epoch,  mse :  5 0.5\n",
            "epoch,  mse :  6 0.5\n",
            "epoch,  mse :  7 0.5\n",
            "epoch,  mse :  8 0.5\n",
            "epoch,  mse :  9 0.5\n",
            "epoch,  mse :  10 0.5\n",
            "epoch,  mse :  11 0.5\n",
            "epoch,  mse :  12 0.5\n",
            "epoch,  mse :  13 0.5\n",
            "epoch,  mse :  14 0.5\n",
            "epoch,  mse :  15 0.5\n",
            "epoch,  mse :  16 0.5\n",
            "epoch,  mse :  17 0.5\n",
            "epoch,  mse :  18 0.5\n",
            "epoch,  mse :  19 0.5\n",
            "epoch,  mse :  20 0.5\n",
            "epoch,  mse :  21 0.5\n",
            "epoch,  mse :  22 0.5\n",
            "epoch,  mse :  23 0.5\n",
            "epoch,  mse :  24 0.5\n",
            "epoch,  mse :  25 0.5\n",
            "epoch,  mse :  26 0.5\n",
            "epoch,  mse :  27 0.5\n",
            "epoch,  mse :  28 0.5\n",
            "epoch,  mse :  29 0.5\n",
            "epoch,  mse :  30 0.5\n",
            "epoch,  mse :  31 0.5\n",
            "epoch,  mse :  32 0.5\n",
            "epoch,  mse :  33 0.5\n",
            "epoch,  mse :  34 0.5\n",
            "epoch,  mse :  35 0.5\n",
            "epoch,  mse :  36 0.5\n",
            "epoch,  mse :  37 0.5\n",
            "epoch,  mse :  38 0.5\n",
            "epoch,  mse :  39 0.5\n",
            "epoch,  mse :  40 0.5\n",
            "epoch,  mse :  41 0.5\n",
            "epoch,  mse :  42 0.5\n",
            "epoch,  mse :  43 0.5\n",
            "epoch,  mse :  44 0.5\n",
            "epoch,  mse :  45 0.5\n",
            "epoch,  mse :  46 0.5\n",
            "epoch,  mse :  47 0.5\n",
            "epoch,  mse :  48 0.5\n",
            "epoch,  mse :  49 0.5\n",
            "====Run 13====\n",
            "epoch,  mse :  0 0.25\n",
            "epoch,  mse :  1 0.25\n",
            "epoch,  mse :  2 0.25\n",
            "epoch,  mse :  3 0.25\n",
            "epoch,  mse :  4 0.25\n",
            "epoch,  mse :  5 0.25\n",
            "epoch,  mse :  6 0.25\n",
            "epoch,  mse :  7 0.25\n",
            "epoch,  mse :  8 0.25\n",
            "epoch,  mse :  9 0.25\n",
            "epoch,  mse :  10 0.25\n",
            "epoch,  mse :  11 0.25\n",
            "epoch,  mse :  12 0.25\n",
            "epoch,  mse :  13 0.25\n",
            "epoch,  mse :  14 0.25\n",
            "epoch,  mse :  15 0.25\n",
            "epoch,  mse :  16 0.25\n",
            "epoch,  mse :  17 0.25\n",
            "epoch,  mse :  18 0.25\n",
            "epoch,  mse :  19 0.25\n",
            "epoch,  mse :  20 0.25\n",
            "epoch,  mse :  21 0.25\n",
            "epoch,  mse :  22 0.25\n",
            "epoch,  mse :  23 0.25\n",
            "epoch,  mse :  24 0.25\n",
            "epoch,  mse :  25 0.25\n",
            "epoch,  mse :  26 0.25\n",
            "epoch,  mse :  27 0.25\n",
            "epoch,  mse :  28 0.25\n",
            "epoch,  mse :  29 0.25\n",
            "epoch,  mse :  30 0.25\n",
            "epoch,  mse :  31 0.25\n",
            "epoch,  mse :  32 0.25\n",
            "epoch,  mse :  33 0.25\n",
            "epoch,  mse :  34 0.25\n",
            "epoch,  mse :  35 0.25\n",
            "epoch,  mse :  36 0.25\n",
            "epoch,  mse :  37 0.25\n",
            "epoch,  mse :  38 0.25\n",
            "epoch,  mse :  39 0.25\n",
            "epoch,  mse :  40 0.25\n",
            "epoch,  mse :  41 0.25\n",
            "epoch,  mse :  42 0.25\n",
            "epoch,  mse :  43 0.25\n",
            "epoch,  mse :  44 0.25\n",
            "epoch,  mse :  45 0.25\n",
            "epoch,  mse :  46 0.25\n",
            "epoch,  mse :  47 0.25\n",
            "epoch,  mse :  48 0.25\n",
            "epoch,  mse :  49 0.25\n",
            "====Run 14====\n",
            "epoch,  mse :  0 0.5\n",
            "epoch,  mse :  1 0.5\n",
            "epoch,  mse :  2 0.5\n",
            "epoch,  mse :  3 0.5\n",
            "epoch,  mse :  4 0.5\n",
            "epoch,  mse :  5 0.5\n",
            "epoch,  mse :  6 0.5\n",
            "epoch,  mse :  7 0.5\n",
            "epoch,  mse :  8 0.5\n",
            "epoch,  mse :  9 0.5\n",
            "epoch,  mse :  10 0.5\n",
            "epoch,  mse :  11 0.5\n",
            "epoch,  mse :  12 0.5\n",
            "epoch,  mse :  13 0.5\n",
            "epoch,  mse :  14 0.5\n",
            "epoch,  mse :  15 0.5\n",
            "epoch,  mse :  16 0.5\n",
            "epoch,  mse :  17 0.5\n",
            "epoch,  mse :  18 0.5\n",
            "epoch,  mse :  19 0.5\n",
            "epoch,  mse :  20 0.5\n",
            "epoch,  mse :  21 0.5\n",
            "epoch,  mse :  22 0.5\n",
            "epoch,  mse :  23 0.5\n",
            "epoch,  mse :  24 0.5\n",
            "epoch,  mse :  25 0.5\n",
            "epoch,  mse :  26 0.5\n",
            "epoch,  mse :  27 0.5\n",
            "epoch,  mse :  28 0.5\n",
            "epoch,  mse :  29 0.5\n",
            "epoch,  mse :  30 0.5\n",
            "epoch,  mse :  31 0.5\n",
            "epoch,  mse :  32 0.5\n",
            "epoch,  mse :  33 0.5\n",
            "epoch,  mse :  34 0.5\n",
            "epoch,  mse :  35 0.5\n",
            "epoch,  mse :  36 0.5\n",
            "epoch,  mse :  37 0.5\n",
            "epoch,  mse :  38 0.5\n",
            "epoch,  mse :  39 0.5\n",
            "epoch,  mse :  40 0.5\n",
            "epoch,  mse :  41 0.5\n",
            "epoch,  mse :  42 0.5\n",
            "epoch,  mse :  43 0.5\n",
            "epoch,  mse :  44 0.5\n",
            "epoch,  mse :  45 0.5\n",
            "epoch,  mse :  46 0.5\n",
            "epoch,  mse :  47 0.5\n",
            "epoch,  mse :  48 0.5\n",
            "epoch,  mse :  49 0.5\n",
            "====Run 15====\n",
            "epoch,  mse :  0 0.0\n",
            "XOR solved!\n",
            "1  epoch(s)\n",
            "C,O  [0 1 1 0] [0. 1. 1. 0.]\n",
            "solved :) ---- Congratulation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyHhorzbby_U",
        "outputId": "98bf7f98-900c-4201-f16c-f5c950960e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[array([[-0.95691775,  0.8294846 , -1.18851741, -0.53512559, -0.91651435,\n",
            "         0.88601639,  0.5790244 ,  1.21443579],\n",
            "       [ 1.17259929, -2.23668276, -0.59871541, -0.53053317, -1.31197937,\n",
            "        -0.68945561,  1.43342449, -0.69130757]]), array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]]), 'relu'], [array([[ 1.04071928,  0.18207764, -1.11481444,  0.54811562],\n",
            "       [-0.30642881, -0.88869415,  0.03271936,  0.80399567],\n",
            "       [ 0.84379842,  0.77141416, -0.95992791,  1.69876138],\n",
            "       [-0.41187246, -0.99713006, -1.01518132, -0.86495941],\n",
            "       [ 0.56311292, -0.34755016, -0.51849771,  0.81404947],\n",
            "       [-2.0737357 , -2.32060859, -0.62120594,  0.21312686],\n",
            "       [ 0.03123729, -0.81695037, -0.39224925,  0.58916436],\n",
            "       [-0.54511107,  0.3208655 ,  1.03088025, -0.24202343]]), array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]]), 'relu'], [array([[ 0.64189024,  1.0511872 ],\n",
            "       [-1.35690004, -0.9769583 ],\n",
            "       [-0.34160059,  0.66549485],\n",
            "       [ 1.34706651, -0.42557442]]), array([[0.],\n",
            "       [0.]]), 'relu'], [array([[-0.83577513],\n",
            "       [ 1.62303733]]), array([[0.]]), 'relu']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call prediction function of the model\n",
        "prob = model_predict(best_model, X, False)\n",
        "print('Prediction probs {}'.format(prob))\n",
        "dec = model_predict(best_model, X, True)\n",
        "print('Model decision {}'.format(dec))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC8iYNHpb0hR",
        "outputId": "1b9a8da0-d791-4f4d-b691-1958faa3061b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction probs [0. 1. 1. 0.]\n",
            "Model decision [0. 1. 1. 0.]\n"
          ]
        }
      ]
    }
  ]
}